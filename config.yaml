Dataset:
  dataset: coco
  root: "/mydataset"
  train_name: "train_non_empty"
  val_name: "val_non_empty"
  test_name: null
  image_size: [512, 512, 3]
Model:
  backbone:
    name: "resnet_34"
    pretrained: True
    freeze_bn: True
  anchors:
    scales: [1, 1.2599210498948732, 1.5874010519681994]
    ratios: [0.5, 1, 2]
    sizes: [32, 64, 128, 256, 512]
    strides: [8, 16, 32, 64, 128]
    mean: [0, 0, 0, 0]
    std: [0.1, 0.1, 0.2, 0.2]
  FPN:
    pyramid_levels: [3, 4, 5, 6, 7]
    channels: 256
    upsample: "bilinear"
  head:
    classification:
      num_classes: 1
      n_repeat: 4
      use_bn: False
      loss:
        name: "focalloss"
        params:
          alpha: 0.25
          gamma: 2.0
      classification_bias_prior: 0.01
    regression:
      n_repeat: 4
      use_bn: False
      loss:
        name: "smooth_l1_loss"
        params:
          beta: 0.1
Trainer:
  logdir: "./logs"
  num_epochs: 100
  batch_size:
    train: 32
    val: 32
    test: 32
  optimizer:
    name: "torch.optim.Adam"
    params:
      lr: 1e-5
      betas: [0.9, 0.999]
      weight_decay: 1e-6
  scheduler:
    name: "torch.optim.lr_scheduler.OneCycleLR"
    params:
      max_lr: ${Trainer.optimizer.params.lr}
      anneal_strategy: 'cos'
      pct_start: 1e-5
      div_factor: 10
      final_div_factor: 1e4
  tpus: 0
  gpus: 1
  workers: 4
  clip_grad_norm: 0.1
  amp: False
  amp_backend: "native"
  num_sanity_val_steps: 0
  callbacks:
    checkpoint:
      enabled: True
      save_top_k: 1
      monitor: 'val_loss'
      mode: 'auto'
      verbose: False
    early_stopping:
      enabled: False
      monitor: 'val_loss'
      mode: 'auto'
      patience: 5
      verbose: False
    lr_monitor:
      enabled: False
      logging_interval: null




